{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJjnhOLDmjH2"
   },
   "source": [
    "# Dataset Introduction - the Project Knowledge Graph\n",
    "\n",
    "Ruijie Wang, Pascal Severin Andermatt | 28-09-2022  \n",
    "Matthias Baumgartner, Luca Rossetto, Cristina Sarasua | Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 12393,
     "status": "ok",
     "timestamp": 1632683772579,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "qsEynPSV4FKf",
    "outputId": "b0a602e6-abbd-497a-ad9d-abbac38a65ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
    "from rdflib.term import URIRef, Literal\n",
    "import csv\n",
    "import json\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "from collections import defaultdict, Counter\n",
    "import locale\n",
    "_ = locale.setlocale(locale.LC_ALL, '')\n",
    "from _plotly_future_ import v4_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "#NER\n",
    "from transformers import pipeline, set_seed\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import create_optimizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "import tensorflow as tf\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "import editdistance\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53nBQpYb37bw"
   },
   "source": [
    "## 1. Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBCtW2krgAxV"
   },
   "source": [
    "### 1.1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139148,
     "status": "ok",
     "timestamp": 1632681991550,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "lJqbjO9D4TcN",
    "outputId": "49fe0321-0ef5-4324-d56a-8aba54913547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N8b5f0edc53f64084b28e0ea676b2217e (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = rdflib.Graph()\n",
    "graph.parse('./dataset/14_graph.nt', format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBNRtXqbb0GN"
   },
   "source": [
    "### 1.2 Graph Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1632682133729,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "7KQagm9qcEFN"
   },
   "outputs": [],
   "source": [
    "# prefixes used in the graph\n",
    "WD = Namespace('http://www.wikidata.org/entity/')\n",
    "WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
    "SCHEMA = Namespace('http://schema.org/')\n",
    "DDIS = Namespace('http://ddis.ch/atai/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-saved P values for Wikidata movies graph\n",
    "global P_values, Q_values\n",
    "P_values = {\n",
    "    'director':'P57',\n",
    "    'cast':'P161',\n",
    "    'producer':'P162',\n",
    "    'genre':'P136',\n",
    "    'character':'P674',\n",
    "    'screenwriter':'P58',   \n",
    "    'filming location':'P915',\n",
    "    'IMDB Id':'P345',\n",
    "    'image':'P18'\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "Q_values = {\n",
    "    'fictional human':'Q15632617',\n",
    "    'film':'Q11424',\n",
    "    'human':'Q5'\n",
    " \n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbvSdhD3d3M1"
   },
   "source": [
    "### 1.3 External Resource Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 5533,
     "status": "ok",
     "timestamp": 1632682466047,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "ZqeAnA7scSLX",
    "outputId": "081c756b-63e8-4cf1-c1d0-c676a2da31fa"
   },
   "outputs": [],
   "source": [
    "\n",
    "top250 = set(open('../dataset/imdb-top-250.t').read().split('\\n')) - {''}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPK1SDy_eScM"
   },
   "source": [
    "### 1.4 Literal Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1632684321965,
     "user": {
      "displayName": "Go Friday",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04257507274310331112"
     },
     "user_tz": -120
    },
    "id": "1E8tkGhIkZ93"
   },
   "outputs": [],
   "source": [
    "roots = {\n",
    "    WD['Q8242']:        'literature',\n",
    "    WD['Q5']:           'human',\n",
    "    WD['Q483394']:      'genre',\n",
    "    WD['Q95074']:       'character',\n",
    "    WD['Q11424']:       'film',\n",
    "    WD['Q15416']:       'tv',\n",
    "    WD['Q618779']:      'award',\n",
    "    WD['Q27096213']:    'geographic',\n",
    "    WD['Q43229']:       'organisation',\n",
    "    WD['Q34770']:       'language',\n",
    "    WD['Q7725310']:     'series',\n",
    "    WD['Q47461344']:    'written work',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9xzeSuXk5eK"
   },
   "source": [
    "## 3. SPARQL query examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P57 - director of film\n",
    "P31 - instance of\n",
    "Q11424 - film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/P1657\n",
      "http://www.wikidata.org/entity/Q134773\n",
      "http://www.wikidata.org/prop/direct/P57\n"
     ]
    }
   ],
   "source": [
    "def find_entity_given_label(entity_label, entity_type=\"none\"):\n",
    "    \n",
    "    entity_label = \"\\\"\" + str(entity_label) + \"\\\"@en\"\n",
    "    \n",
    "    \n",
    "  \n",
    "    if entity_type == \"none\":\n",
    "        query_content = \"\"\"PREFIX ddis: <http://ddis.ch/atai/> \n",
    "        PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "        PREFIX schema: <http://schema.org/> \n",
    "\n",
    "        SELECT ?entity WHERE {{\n",
    "            ?entity rdfs:label {}\n",
    "        }} \"\"\".format(entity_label)\n",
    "    else:\n",
    "        query_content = \"\"\"PREFIX ddis: <http://ddis.ch/atai/> \n",
    "        PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "        PREFIX schema: <http://schema.org/> \n",
    "\n",
    "        SELECT ?entity WHERE {{\n",
    "            ?entity rdfs:label {} .\n",
    "            ?entity wdt:P31 wd:{} .\n",
    "        }} \"\"\".format(entity_label, entity_type)\n",
    "        \n",
    "    \n",
    "#     print(query_content)\n",
    "    res =  list(graph.query(query_content))\n",
    "    if len(res)>0:\n",
    "        return res[0][0]\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "print(find_entity_given_label(\"MPAA film rating\") )\n",
    "\n",
    "\n",
    "print(find_entity_given_label(\"Forrest Gump\",'Q11424') )\n",
    "print(find_entity_given_label(\"director\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX ddis: <http://ddis.ch/atai/> \n",
      "    PREFIX wd: <http://www.wikidata.org/entity/> \n",
      "    PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
      "    PREFIX schema: <http://schema.org/> \n",
      "    \n",
      "    SELECT ?answer WHERE {\n",
      "     ?movie rdfs:label \"Forrest Gump\"@en .\n",
      "     ?movie wdt:P31 wd:Q11424 .\n",
      "     ?movie wdt:P57 ?answer\n",
      "    } \n",
      "(rdflib.term.URIRef('http://www.wikidata.org/entity/Q187364'),)\n"
     ]
    }
   ],
   "source": [
    "def query_something_about_movie(p_val, label):\n",
    "    \n",
    "    query_content = \"\"\"PREFIX ddis: <http://ddis.ch/atai/> \n",
    "    PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "    PREFIX schema: <http://schema.org/> \n",
    "    \n",
    "    SELECT ?answer WHERE {{\n",
    "     ?movie rdfs:label \"{}\"@en .\n",
    "     ?movie wdt:P31 wd:Q11424 .\n",
    "     ?movie wdt:{} ?answer\n",
    "    }} \"\"\".format(label, p_val)\n",
    "    \n",
    "    print(query_content)\n",
    "    return list(graph.query(query_content))\n",
    "      \n",
    "a = query_something_about_movie(P_values['director'], 'Forrest Gump' )    \n",
    "  \n",
    "for i in a:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX ddis: <http://ddis.ch/atai/> \n",
      "                        PREFIX wd: <http://www.wikidata.org/entity/> \n",
      "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
      "                        PREFIX schema: <http://schema.org/> \n",
      "\n",
      "                        SELECT ?label WHERE {\n",
      "                         <http://www.wikidata.org/entity/Q187364> rdfs:label ?label .\n",
      "                         \n",
      "                        } \n",
      "(rdflib.term.Literal('Robert Zemeckis', lang='en'),)\n"
     ]
    }
   ],
   "source": [
    "def get_label_of_Qval(q_val):\n",
    "    \n",
    "    query_content =  \"\"\"PREFIX ddis: <http://ddis.ch/atai/> \n",
    "                        PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "                        PREFIX schema: <http://schema.org/> \n",
    "\n",
    "                        SELECT ?label WHERE {{\n",
    "                         <{}> rdfs:label ?label .\n",
    "                         \n",
    "                        }} \"\"\".format(q_val)\n",
    "    \n",
    "    print(query_content)\n",
    "    return list(graph.query(query_content))\n",
    "\n",
    "a = get_label_of_Qval('http://www.wikidata.org/entity/Q187364')\n",
    "  \n",
    "for i in a:\n",
    "    print(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q187364\n"
     ]
    }
   ],
   "source": [
    "def find_something_about_an_entity(entity_URI, relation_URI):\n",
    "    \n",
    "   \n",
    "    \n",
    "    query_content = \"\"\"PREFIX ddis: <http://ddis.ch/atai/> \n",
    "    PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "    PREFIX schema: <http://schema.org/> \n",
    "    \n",
    "    SELECT ?res WHERE {{\n",
    "        <{}> <{}> ?res\n",
    "        \n",
    "    }} \"\"\".format(entity_URI, relation_URI)\n",
    "    \n",
    "\n",
    "    res =  list(graph.query(query_content))\n",
    "    return res[0][0]\n",
    "a = find_something_about_an_entity('http://www.wikidata.org/entity/Q134773','http://www.wikidata.org/prop/direct/P57') \n",
    "\n",
    "# for elements in a[0]:\n",
    "#     print(elements)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24384\n",
      "['Jan Dara', 'Moondram Pirai', \"Buffalo Bill and the Indians, or Sitting Bull's History Lesson\", 'What We Wanted', 'Wanted: Dead or Alive']\n"
     ]
    }
   ],
   "source": [
    "def write_list_to_file(list_name, file_name):\n",
    "    with open(file_name, 'w', encoding=\"utf-8\") as filehandle:\n",
    "        for listitem in list_name:\n",
    "            filehandle.write(f'{listitem}\\n')\n",
    "        \n",
    "def read_list_from_file(file_name):\n",
    "    res_list = []\n",
    "    with open(file_name, 'r', encoding=\"utf-8\") as filehandle:\n",
    "        for line in filehandle:\n",
    "            curr_place = line[:-1]\n",
    "            res_list.append(curr_place)\n",
    "    return res_list\n",
    "\n",
    "def get_all_movies(write_file_pathname):\n",
    "    query_content = \"\"\"PREFIX ddis: <http://ddis.ch/atai/> \n",
    "    PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "    PREFIX schema: <http://schema.org/> \n",
    "    \n",
    "    SELECT ?label WHERE {{\n",
    "        ?movie rdfs:label ?label .\n",
    "        ?movie wdt:P31 wd:Q11424 .\n",
    "        \n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    res = list(graph.query(query_content))\n",
    "    res_list = []\n",
    "    for i in res:\n",
    "        res_list.append(str(i[0]))\n",
    "    write_list_to_file(res_list, write_file_pathname)\n",
    "    \n",
    "get_all_movies(\"save_files/all_movies_list.txt\")\n",
    "\n",
    "movies_list = []\n",
    "movies_list = read_list_from_file(\"save_files/all_movies_list.txt\")\n",
    "print(len(movies_list))\n",
    "print(movies_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100157\n",
      "['Viktor Krištof', 'Yuji Nomi', 'Béatrice Thiriet', 'Oleg Kapanets', 'Ram Lee']\n"
     ]
    }
   ],
   "source": [
    "def get_all_humans(write_file_pathname):\n",
    "    query_content = \"\"\"PREFIX ddis: <http://ddis.ch/atai/> \n",
    "    PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "    PREFIX schema: <http://schema.org/> \n",
    "    \n",
    "    SELECT ?label WHERE {{\n",
    "        ?person rdfs:label ?label .\n",
    "        ?person wdt:P31 wd:Q5 .\n",
    "        \n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    res = list(graph.query(query_content))\n",
    "    res_list = []\n",
    "    for i in res:\n",
    "        res_list.append(str(i[0]))\n",
    "    write_list_to_file(res_list, write_file_pathname)\n",
    "    \n",
    "get_all_humans(\"save_files/all_humans_list.txt\")\n",
    "humans_list = []\n",
    "humans_list = read_list_from_file(\"save_files/all_humans_list.txt\")\n",
    "print(len(humans_list))\n",
    "print(humans_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'res': \"Buffalo Bill and the Indians, or Sitting Bull's History Lesson\", 'res_ind': 2}\n",
      "{'res': 'Béatrice Thiriet', 'res_ind': 2}\n",
      "{'res': 'Triple Threat', 'res_ind': 10650}\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def find_closest_match_in_a_List(word, target_list):\n",
    "    res = difflib.get_close_matches(word.lower(), [item.lower() for item in target_list], n=1, cutoff = 0.6)\n",
    "    res_ind = -1\n",
    "    \n",
    "    if len(res)!=0:\n",
    "        for i in range(len(target_list)):\n",
    "            if (target_list[i].lower()) == res[0]:\n",
    "                res_ind = i\n",
    "                res = target_list[i]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "#     print(res)\n",
    "#     print(res_ind)\n",
    "    return {'res':res, 'res_ind':res_ind}\n",
    "print(find_closest_match_in_a_List('BuffaloBill and the Indians', movies_list))\n",
    "print(find_closest_match_in_a_List('Beatrice Thiriet', humans_list))\n",
    "print(find_closest_match_in_a_List('Beatrice Thiriet', movies_list))\n",
    "print(find_closest_match_in_a_List('Beatrice Thiriet', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q187364\n",
      "http://www.wikidata.org/entity/Q102427\n"
     ]
    }
   ],
   "source": [
    "def deal_with_KG_query(entity, relation):\n",
    "    \n",
    "    # Process Entity \n",
    "    \n",
    "    match_list = []\n",
    "    entity_type = 'none'\n",
    "    movie_res = find_closest_match_in_a_List(entity, movies_list)\n",
    "    if movie_res != -1:\n",
    "        match_list.append(movie_res['res'])\n",
    "    human_res = find_closest_match_in_a_List(entity, humans_list)\n",
    "    if human_res != -1:\n",
    "        match_list.append(human_res['res'])\n",
    "\n",
    "    final_entity_res = find_closest_match_in_a_List(entity, match_list)\n",
    "    \n",
    "    if final_entity_res == -1:\n",
    "        final_entity_res = {'res':entity, 'res_ind' : -1}\n",
    "        \n",
    "    else:\n",
    "        if (final_entity_res['res'] == human_res['res']):\n",
    "            entity_type = 'human'\n",
    "        elif (final_entity_res['res'] == movie_res['res']):\n",
    "            entity_type = 'film'\n",
    "        \n",
    "#     print(final_entity_res['res'])\n",
    "#     print(entity_type)\n",
    "    entity_URI = find_entity_given_label(final_entity_res['res'], Q_values[entity_type])\n",
    "    \n",
    "    \n",
    "    # Process Relation \n",
    "    \n",
    "    entity_type = \"none\"\n",
    "    relation_list = list(P_values.keys())\n",
    "    \n",
    "    final_relation_res = find_closest_match_in_a_List(relation, relation_list)\n",
    "    if final_relation_res == -1:\n",
    "        final_relation_res = {'res':relation, 'res_ind' : -1}\n",
    "#     print(final_relation_res['res'])\n",
    "#     print(entity_type)\n",
    "    relation_URI = find_entity_given_label(final_relation_res['res'], entity_type)\n",
    "    \n",
    "#     print(relation_URI)\n",
    "#     print(entity_URI)\n",
    "    \n",
    "    return find_something_about_an_entity(entity_URI, relation_URI)\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "res = deal_with_KG_query('Forest Gump','dicrector')\n",
    "print(res)\n",
    "\n",
    "res = deal_with_KG_query('Forest Gump','nominated for')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdflib.term.URIRef('http://www.wikidata.org/entity/Q134773')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_entity_given_label('Forrest Gump', Q_values['film'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/prop/direct/P18\n"
     ]
    }
   ],
   "source": [
    "print(WDT['P18'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = [\"Who is the director of Good Will Hunting?\", \"Who directed The Bridge on the River Kwai?\", \n",
    "                    \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\", \"Who is the screenwriter of The Masked Gang: Cyprus?\",\n",
    "                    \"What is the MPAA film rating of Weathering with You?\", \"What is the genre of Good Neighbors?\", \"Show me a picture of Halle Berry.\",\n",
    "                    \"What does Julia Roberts look like?\", \"Let me know what Sandra Bullock looks like.\", \"Recommend movies similar to Hamlet and Othello.\",\n",
    "                    \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\",\n",
    "                    \"Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\",\n",
    "                    \"Can you tell me the publication date of Tom Meets Zizou?\", \"Who is the executive producer of X-Men: First Class?\",\n",
    "                    \"Who is the Director of Batman 1989?\", \"What is the box office of The Princess and the Frog?\",\n",
    "                   \"What is the birthplace of Christopher Nolan?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'Who is the director of Good Will Hunting?', 'type': ''},\n",
       " {'query': 'Who directed The Bridge on the River Kwai?', 'type': ''},\n",
       " {'query': 'Who is the director of Star Wars: Episode VI - Return of the Jedi?',\n",
       "  'type': ''},\n",
       " {'query': 'Who is the screenwriter of The Masked Gang: Cyprus?', 'type': ''},\n",
       " {'query': 'What is the MPAA film rating of Weathering with You?', 'type': ''},\n",
       " {'query': 'What is the genre of Good Neighbors?', 'type': ''},\n",
       " {'query': 'Show me a picture of Halle Berry.', 'type': ''},\n",
       " {'query': 'What does Julia Roberts look like?', 'type': ''},\n",
       " {'query': 'Let me know what Sandra Bullock looks like.', 'type': ''},\n",
       " {'query': 'Recommend movies similar to Hamlet and Othello.', 'type': ''},\n",
       " {'query': 'Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?',\n",
       "  'type': ''},\n",
       " {'query': 'Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.',\n",
       "  'type': ''},\n",
       " {'query': 'Can you tell me the publication date of Tom Meets Zizou?',\n",
       "  'type': ''},\n",
       " {'query': 'Who is the executive producer of X-Men: First Class?', 'type': ''},\n",
       " {'query': 'Who is the Director of Batman 1989?', 'type': ''},\n",
       " {'query': 'What is the box office of The Princess and the Frog?', 'type': ''},\n",
       " {'query': 'What is the birthplace of Christopher Nolan?', 'type': ''}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df = [{\"query\": s, \"type\" : \"\"}for s in sample_questions]\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_POS = AutoTokenizer.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
    "model_POS = model = AutoModelForTokenClassification.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Who is the director of Good Will Hunting?',\n",
       " 'type': '',\n",
       " 'pos': [{'entity_group': 'PRON',\n",
       "   'score': 0.99944025,\n",
       "   'word': 'who',\n",
       "   'start': 0,\n",
       "   'end': 3},\n",
       "  {'entity_group': 'AUX',\n",
       "   'score': 0.9970728,\n",
       "   'word': 'is',\n",
       "   'start': 4,\n",
       "   'end': 6},\n",
       "  {'entity_group': 'DET',\n",
       "   'score': 0.99954873,\n",
       "   'word': 'the',\n",
       "   'start': 7,\n",
       "   'end': 10},\n",
       "  {'entity_group': 'NOUN',\n",
       "   'score': 0.986273,\n",
       "   'word': 'director',\n",
       "   'start': 11,\n",
       "   'end': 19},\n",
       "  {'entity_group': 'ADP',\n",
       "   'score': 0.9994654,\n",
       "   'word': 'of',\n",
       "   'start': 20,\n",
       "   'end': 22},\n",
       "  {'entity_group': 'PROPN',\n",
       "   'score': 0.6715452,\n",
       "   'word': 'good',\n",
       "   'start': 23,\n",
       "   'end': 27},\n",
       "  {'entity_group': 'NOUN',\n",
       "   'score': 0.6608364,\n",
       "   'word': 'will',\n",
       "   'start': 28,\n",
       "   'end': 32},\n",
       "  {'entity_group': 'PROPN',\n",
       "   'score': 0.77655363,\n",
       "   'word': 'hunting',\n",
       "   'start': 33,\n",
       "   'end': 40},\n",
       "  {'entity_group': 'PUNCT',\n",
       "   'score': 0.9996462,\n",
       "   'word': '?',\n",
       "   'start': 40,\n",
       "   'end': 41}]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_POS, aggregation_strategy=\"simple\", tokenizer = tokenizer_POS\n",
    ")\n",
    "\n",
    "# POS tagging for all of the questions\n",
    "for i in range (len(questions_df)):\n",
    "    questions_df[i][\"pos\"] = token_classifier(questions_df[i][\"query\"])\n",
    "questions_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the type of question by keyword matching\n",
    "def find_type(formulated_question_df):\n",
    "    keywords_images = [ 'image', 'picture', 'look', 'looks' ]\n",
    "    keywords_recommendation = ['similar', 'recommend', 'recommendations']\n",
    "    res_type = ''\n",
    "    query_list = []\n",
    "    for i in formulated_question_df['pos']:\n",
    "        query_list.append(i['word']) \n",
    "    if any(word in query_list for word in keywords_images):\n",
    "        res_type = \"images\"\n",
    "    elif any(word in query_list for word in keywords_recommendation):\n",
    "        res_type = \"recommendation\"\n",
    "    else :\n",
    "        res_type = \"search\"\n",
    "            \n",
    "    return res_type\n",
    "\n",
    "# Add the type for all questions\n",
    "for i in range (len(questions_df)):\n",
    "    questions_df[i]['type'] = find_type(questions_df[i])\n",
    "    \n",
    "#     print(questions_df[i]['query']) \n",
    "#     print(questions_df[i]['type']) \n",
    "#     print(\"______\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Who directed The Bridge on the River Kwai?',\n",
       " 'type': 'search',\n",
       " 'pos': [{'entity_group': 'PRON',\n",
       "   'score': 0.9994,\n",
       "   'word': 'who',\n",
       "   'start': 0,\n",
       "   'end': 3},\n",
       "  {'entity_group': 'VERB',\n",
       "   'score': 0.99945265,\n",
       "   'word': 'directed',\n",
       "   'start': 4,\n",
       "   'end': 12},\n",
       "  {'entity_group': 'DET',\n",
       "   'score': 0.99950266,\n",
       "   'word': 'the',\n",
       "   'start': 13,\n",
       "   'end': 16},\n",
       "  {'entity_group': 'NOUN',\n",
       "   'score': 0.9979869,\n",
       "   'word': 'bridge',\n",
       "   'start': 17,\n",
       "   'end': 23},\n",
       "  {'entity_group': 'ADP',\n",
       "   'score': 0.9993905,\n",
       "   'word': 'on',\n",
       "   'start': 24,\n",
       "   'end': 26},\n",
       "  {'entity_group': 'DET',\n",
       "   'score': 0.99918383,\n",
       "   'word': 'the',\n",
       "   'start': 27,\n",
       "   'end': 30},\n",
       "  {'entity_group': 'PROPN',\n",
       "   'score': 0.9451487,\n",
       "   'word': 'river kwai',\n",
       "   'start': 31,\n",
       "   'end': 41},\n",
       "  {'entity_group': 'PUNCT',\n",
       "   'score': 0.9996606,\n",
       "   'word': '?',\n",
       "   'start': 41,\n",
       "   'end': 42}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the entity for a question of type images\n",
    "def get_entity_for_images(formulated_question_df):\n",
    "    adp_ind = -1\n",
    "\n",
    "    for i ,pos_res in enumerate(formulated_question_df['pos']):\n",
    "        if pos_res['entity_group'] == 'PROPN':\n",
    "            return pos_res['word']\n",
    "    return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "Halle Berry\n",
      "http://www.wikidata.org/entity/Q1033016\n",
      "https://commons.wikimedia.org/wiki/File:Halle_Berry_by_Gage_Skidmore_2.jpg\n",
      "Julia Roberts\n",
      "http://www.wikidata.org/entity/Q40523\n",
      "https://commons.wikimedia.org/wiki/File:Julia_Roberts_(43838880775).jpg\n",
      "Sandra Bullock\n",
      "http://www.wikidata.org/entity/Q40791\n",
      "https://commons.wikimedia.org/wiki/File:Sandra_Bullock,_The_Heat,_London,_2013_(crop).jpg\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# main function to answer image questions\n",
    "def handle_image_questions(formulated_question_df):\n",
    "    if formulated_question_df['type']!='images':\n",
    "        return -1\n",
    "    else:\n",
    "        name = get_entity_for_images(formulated_question_df)\n",
    "       \n",
    "        \n",
    "    match_list = []\n",
    "    entity_type = 'none'\n",
    "    movie_res = find_closest_match_in_a_List(name, movies_list)\n",
    "    if movie_res != -1:\n",
    "        match_list.append(movie_res['res'])\n",
    "    human_res = find_closest_match_in_a_List(name, humans_list)\n",
    "    if human_res != -1:\n",
    "        match_list.append(human_res['res'])\n",
    "\n",
    "    final_entity_res = find_closest_match_in_a_List(name, match_list)\n",
    "    \n",
    "    if final_entity_res == -1:\n",
    "        final_entity_res = {'res':entity, 'res_ind' : -1}\n",
    "        \n",
    "    else:\n",
    "        if (final_entity_res['res'] == human_res['res']):\n",
    "            entity_type = 'human'\n",
    "        elif (final_entity_res['res'] == movie_res['res']):\n",
    "            entity_type = 'film'\n",
    "    print(final_entity_res['res'])\n",
    "    entity_URI = find_entity_given_label(final_entity_res['res'], Q_values[entity_type])\n",
    "    \n",
    "    print(entity_URI)\n",
    "    \n",
    "    return find_something_about_an_entity(entity_URI, WDT['P18'])\n",
    "    \n",
    "    \n",
    "for q in questions_df:\n",
    "    res = handle_image_questions(q)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_relation_for_search(formulated_question_df):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "# labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(    \"vblagoje/bert-english-uncased-finetuned-pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 3394\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 426\n",
      "  Number of trainable parameters = 66372877\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='426' max='426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [426/426 00:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.262370</td>\n",
       "      <td>0.536776</td>\n",
       "      <td>0.317887</td>\n",
       "      <td>0.399302</td>\n",
       "      <td>0.943226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.272362</td>\n",
       "      <td>0.562033</td>\n",
       "      <td>0.348471</td>\n",
       "      <td>0.430206</td>\n",
       "      <td>0.945449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1287\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_wnut_model\\checkpoint-213\n",
      "Configuration saved in my_awesome_wnut_model\\checkpoint-213\\config.json\n",
      "Model weights saved in my_awesome_wnut_model\\checkpoint-213\\pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_wnut_model\\checkpoint-213\\tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_wnut_model\\checkpoint-213\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1287\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_wnut_model\\checkpoint-426\n",
      "Configuration saved in my_awesome_wnut_model\\checkpoint-426\\config.json\n",
      "Model weights saved in my_awesome_wnut_model\\checkpoint-426\\pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_wnut_model\\checkpoint-426\\tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_wnut_model\\checkpoint-426\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from my_awesome_wnut_model\\checkpoint-213 (score: 0.26236963272094727).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=426, training_loss=0.10359365615486539, metrics={'train_runtime': 18.7858, 'train_samples_per_second': 361.336, 'train_steps_per_second': 22.677, 'total_flos': 92090981263080.0, 'train_loss': 0.10359365615486539, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\"\n",
    "classifier = pipeline(\"ner\", model=\"stevhliu/my_awesome_wnut_model\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batman: ORG (0.92)\n"
     ]
    }
   ],
   "source": [
    "entities = ner_pipeline(sample_question, aggregation_strategy=\"simple\")\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']}: {entity['entity_group']} ({entity['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question pattern: who is the (.*) of ENTITY\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'entity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m question_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho is the (.*) of ENTITY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion pattern: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(question_pattern))\n\u001b[1;32m----> 7\u001b[0m question \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[43mentity\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENTITY\u001b[39m\u001b[38;5;124m\"\u001b[39m, question\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# preprocess the question\u001b[39;00m\n\u001b[0;32m      9\u001b[0m relation \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mmatch(question_pattern, question)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# match the relation using a pattern\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognized relation: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(relation))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'entity' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# a naive way for matching entities and relations\n",
    "\n",
    "question_pattern = \"who is the (.*) of ENTITY\"\n",
    "\n",
    "print(\"question pattern: {}\\n\".format(question_pattern))\n",
    "\n",
    "question = re.sub(entity, \"ENTITY\", question.rstrip(\"?\"))  # preprocess the question\n",
    "\n",
    "relation = re.match(question_pattern, question).group(1)  # match the relation using a pattern\n",
    "\n",
    "print(\"recognized relation: {}\\n\".format(relation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "predicates = {}\n",
    "\n",
    "for node in g.all_nodes():\n",
    "    if isinstance(node, URIRef):\n",
    "        if g.value(node, n.label):\n",
    "            nodes[node.toPython()] = g.value(node, n.label).toPython()\n",
    "        else:\n",
    "            nodes[node.toPython()] = re.sub(\"http://example.org/\", \"\", node.toPython())\n",
    "\n",
    "for s, p, o in g:\n",
    "    predicates[p.toPython()] = re.sub(\"http://example.org/\", \"\", p.toPython())\n",
    "\n",
    "print(\"labeled nodes: {}\\n\".format(nodes))\n",
    "print(\"predicates: {}\\n\".format(predicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp = 9999\n",
    "match_node = \"\"\n",
    "print(\"--- entity matching for \\\"{}\\\"\\n\".format(entity))\n",
    "for key, value in nodes.items():\n",
    "    print(\"edit distance between {} and {}: {}\".format(value, entity, editdistance.eval(value, entity)))\n",
    "    if editdistance.eval(value, entity) < tmp:\n",
    "        tmp = editdistance.eval(value, entity)\n",
    "        match_node = key\n",
    "\n",
    "tmp = 9999\n",
    "match_pred = \"\"\n",
    "print(\"\\n--- relation matching for \\\"{}\\\"\\n\".format(relation))\n",
    "for key, value in predicates.items():\n",
    "    print(\"edit distance between {} and {}: {}\".format(value, relation, editdistance.eval(value, relation)))\n",
    "    if editdistance.eval(value, relation) < tmp:\n",
    "        tmp = editdistance.eval(value, relation)\n",
    "        match_pred = key\n",
    "\n",
    "print(\"\\n--- the matching node of \\\"{}\\\" is {}\\n\".format(entity, match_node))\n",
    "print(\"--- the matching predicates of \\\"{}\\\" is {}\\n\".format(relation, match_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"SELECT DISTINCT ?x ?y WHERE {{ ?x <{}> <{}>. ?x <{}> ?y. }}\".format(match_pred, match_node, n.label)\n",
    "\n",
    "print(\"--- sparql query: {}\".format(query_template))\n",
    "\n",
    "qres = g.query(query_template)\n",
    "\n",
    "print(\"\\n--- querying results: \")\n",
    "for row in qres:\n",
    "    print(row.x, row.y)\n",
    "    answer = row.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Sentence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_json = pd.read_json(os.path.join('..','dataset/movienet/images.json')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w</th>\n",
       "      <th>movie</th>\n",
       "      <th>img</th>\n",
       "      <th>h</th>\n",
       "      <th>type</th>\n",
       "      <th>cast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1666</td>\n",
       "      <td>[tt4882376]</td>\n",
       "      <td>0315/rm601699072.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>behind_the_scenes</td>\n",
       "      <td>[nm8801745, nm0001401]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999</td>\n",
       "      <td>[tt2318625]</td>\n",
       "      <td>2538/rm814292736.jpg</td>\n",
       "      <td>562</td>\n",
       "      <td>still_frame</td>\n",
       "      <td>[nm2072214]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500</td>\n",
       "      <td>[tt4003966]</td>\n",
       "      <td>0354/rm2068192512.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>still_frame</td>\n",
       "      <td>[nm0268626]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1333</td>\n",
       "      <td>[]</td>\n",
       "      <td>3777/rm27402752.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>publicity</td>\n",
       "      <td>[nm6655379]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704</td>\n",
       "      <td>[]</td>\n",
       "      <td>3459/rm537652736.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>event</td>\n",
       "      <td>[nm1577190, nm7097953]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      w        movie                    img     h               type  \\\n",
       "0  1666  [tt4882376]   0315/rm601699072.jpg  1000  behind_the_scenes   \n",
       "1   999  [tt2318625]   2538/rm814292736.jpg   562        still_frame   \n",
       "2  1500  [tt4003966]  0354/rm2068192512.jpg  1000        still_frame   \n",
       "3  1333           []    3777/rm27402752.jpg  1000          publicity   \n",
       "4   704           []   3459/rm537652736.jpg  1000              event   \n",
       "\n",
       "                     cast  \n",
       "0  [nm8801745, nm0001401]  \n",
       "1             [nm2072214]  \n",
       "2             [nm0268626]  \n",
       "3             [nm6655379]  \n",
       "4  [nm1577190, nm7097953]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNC2BzrVbq+mNDgGNziLFpm",
   "collapsed_sections": [],
   "name": "dataset_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
